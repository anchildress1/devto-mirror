[
  {
    "title": "Waiting, With Intent: Designing AI Systems for the Long Game ğŸ§­",
    "link": "https://dev.to/anchildress1/waiting-with-intent-designing-ai-systems-for-the-long-game-1abg",
    "date": "2026-01-14T12:22:00Z",
    "content_html": "<blockquote>\n<p>ğŸ¦„ Iâ€™m waiting for AI to mature. Very explicitlyâ€”and yes, mostly impatiently. I donâ€™t even think we're close to imagining the future landscape with AI, and honestly pretending otherwise is neither honest or useful to anyone. This post is my attempt to explain how I think about AI from a dev perspective on a longer horizonâ€”five, maybe even ten years down the road. The tools we have right now are still a very long way away from my baseline expectations, which my AI systems remind me of near constantlyâ€”like when I'm trying to force agent-like functionality out of ChatGPT. <strong>Spoiler:</strong> itâ€™s not designed to handle that.</p>\n\n<p>While Iâ€™m waiting, though, Iâ€™m not disengaged. Iâ€™m definitely tinkeringâ€”sometimes randomly and sometimes just as an unsatisfied AI user whoâ€™s not thrilled with the existing systems. Iâ€™m also busy figuring out what the next problems really look like by diving in and getting my hands dirty. </p>\n\n<p>One of those big challenges is what I keep calling the â€œmemory problem.â€ I've designed a solution for my own personal agent to manage long-term memory. Yesâ€”I'm aware that GitHub is inevitably going to beat me to a viable solution. <em>Again</em>. But I'm one of those people who will attempt to solve a problem first, get it wrong at least ten different times, and <em>then</em> do the research to fill in the knowledge gaps. Now I just have to muster up enough oomph to actually do it. ğŸ‰ğŸ§šâ€â™€ï¸</p>\n</blockquote>\n\n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg4wzwazhknufn2ou2to7.png\" class=\"article-body-image-wrapper\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg4wzwazhknufn2ou2to7.png\" alt=\"Human-crafted, AI-edited badge\" loading=\"lazy\" width=\"200\" height=\"200\"></a></p>\n\n\n<hr>\n\n<h2>\n  <a name=\"first-principles-llm-vs-agent%C2%A0\" href=\"#first-principles-llm-vs-agent%C2%A0\">\n  </a>\n  First Principles: LLM vs AgentÂ ğŸ§©\n</h2>\n\n<p>At some point, if you want any of this AI talk to make sense, you have to step back, align terminology, and separate concepts that keep getting blurred together. An LLM, often called a model, is the generative part of GenAIâ€”it accepts input and generates output. <em>That's it.</em> An agent is the system managing context, memory, and various tools. The agent is responsible for what information the LLM even sees in the first place.</p>\n\n<p>When those two ideas get collapsed into the same thing, everything downstream becomes confused. You canâ€™t reason clearly about limits, costs, or failure modes if you donâ€™t separate generation from data management. Until you draw that line, every other discussion ends up muddy.</p>\n\n\n<hr>\n\n<h2>\n  <a name=\"context-is-the-bottleneck-and-everyone-knows-it%C2%A0\" href=\"#context-is-the-bottleneck-and-everyone-knows-it%C2%A0\">\n  </a>\n  Context Is the Bottleneck (and Everyone Knows It)Â ğŸ•¸ï¸\n</h2>\n\n<p>Once you make the distinction between LLM and agent, the real bottleneck becomes obvious. There is no good way to manage context today, let alone have the agent automate that job effectively. If youâ€™re not fully up to date on the lingo: context includes a whole set of things like instruction files, workspace structure, active files in your IDE, the AI chat history, available tools, and more.</p>\n\n<p>What we have now are very manual tools that do very little to solve the problem. We have to remember to tell the AI which parts currently matterâ€”or at some point we have to clear the chat entirely and start over. If we donâ€™t do that deliberately, AI slowly loses the point of whatâ€™s we're supposed to be working on in the first place. At worst, the entire chat thread is poisoned and the AI becomes unable to function at all. Then you're forced to start fresh and always at the most inconvenient time.</p>\n\n<p>And donâ€™t expect LLM context to scale, either. Hardware costs may go down eventually, but nowhere near fast enough to keep up with everything we keep throwing at it. So, context is very finiteâ€”especially in GitHub where context windows are smaller than normal anyway.</p>\n\n<p>The agent will typically make space by compacting information. ItÂ will ask the LLM to summarize key points and then it literally drops the original full length novel completely from your active context and replaces it with the cliff notes version. The more summarization, the less accurate things get over time. So naturally you retry prompts while adding back the dropped details and you end up making more calls for a single task overall. The model has to process more and more input just to get you back to the same answer you already hadâ€”not necessarily a better one.</p>\n\n<p>People know this is a problem. Tools like <a href=\"https://github.com/toon-format/toon\" target=\"_blank\" rel=\"noopener noreferrer\">Toon</a> exist specifically to minimize input impact for AI. We also have tools like <a href=\"https://docs.github.com/?search-overlay-open=true&amp;search-overlay-input=runSubagent&amp;search-overlay-ask-ai=true\" target=\"_blank\" rel=\"noopener noreferrer\">Copilot's <code>#runSubagent</code></a> to help manage context within a single agent. These aren't true solutions thoughâ€”they are signals. These are the problems people are trying to solve yesterday while we wait for the next AI evolution to emerge.</p>\n\n\n<hr>\n\n<h2>\n  <a name=\"why-orchestration-is-inevitable%C2%A0\" href=\"#why-orchestration-is-inevitable%C2%A0\">\n  </a>\n  Why Orchestration Is InevitableÂ ğŸ™\n</h2>\n\n<p>Even if you do everything â€œrightâ€ and manage context like a master AI sensei, agents eventually hit a limit. The list of must-have MCPs is growing and right now those stay in the context window as long as they're enabled. Projects are starting and accumulating larger knowledge bases. Customization is becoming more and more explicit. The context an agent needs to use will continue to grow exponentially, even though LLMs aren't increasing capacity at the same speed.</p>\n\n<p>The ultimate overflow state isnâ€™t hypotheticalâ€”itâ€™s inevitable. Once an agent accumulates enough memory, enough history, enough summarization, the LLM simply canâ€™t keep up coherently anymore. That isnâ€™t a failure in the systemâ€”itâ€™s a limit.</p>\n\n<p>When you hit that limit, you can't just tweak prompts or optimize harder. You wouldn't try to squeeze more juice out of the same dry orange, either. The only real long-term solution is that you split the systemâ€”<em>you have to</em>!</p>\n\n<p>Smaller pieces of work are then sent to the LLM with only relevant context, which is when smarter agents will start to appear. This is where summarization stops and you retain the original intent at both a high-level and at the lowest-level. When we get here, AI generation stops being the problemâ€”the new problem is coordinating all those tiny pieces of work and still accomplishing the larger goal without re-prompting anything previously stated or defined elsewhere already. <em>Welcome to the world of true agent orchestration</em>!</p>\n\n<blockquote>\n<p>ğŸ’¡ <strong>ProTip:</strong> If you want a sneak peek of what this looks like, check out <a href=\"https://verdent.ai\" target=\"_blank\" rel=\"noopener noreferrer\">Verdent.ai</a>. Of all the solutions I've worked with, Verdent is the only one that's truly designed for agent orchestration. It also excels in VS Code and wins every coding competition I've put it in.</p>\n</blockquote>\n\n\n<hr>\n\n<h2>\n  <a name=\"orchestration-as-a-system-property\" href=\"#orchestration-as-a-system-property\">\n  </a>\n  Orchestration as a System Property â™Ÿï¸\n</h2>\n\n<p>Orchestration isnâ€™t just about sequencing work in a nicer wayâ€”itâ€™s about changing where responsibility lives. Yesâ€”some things are always going to be sequential, but not everything needs to be. Some things can and should run in parallel, especially if you want speed and reliability included in future agentic systems.</p>\n\n<p>Validation is a fundamental part of orchestration, not something bolted on afterward. A successful agent has to be able to verify its own work without relying on prior context. It has to come in like a third party, with no knowledge beyond the repo instructions. CodeQL, lint enforcement, Makefiles, and even extra tests become the ground truth the system must consistently check itself against.</p>\n\n<p>Multi-model opposition fits naturally here, too. Different models trained by different companies catch different things. Then the agent can pick one modelÂ  to implement and another to review. The point is that they disagree by default and then they converge around a common goal. This is a pivotal moment in the future landscape becauseÂ officially the LLM is no longer the center of gravityâ€”the agentic system is.</p>\n\n<blockquote>\n<p>ğŸ¤ <strong>ShoutOut</strong> <a class=\"mentioned-user\" href=\"https://dev.to/marcosomma\">@marcosomma</a> wrote a brilliant article on <a href=\"https://dev.to/marcosomma/loopnode-how-orka-orchestrates-iterated-thought-until-agreement-emerges-17l2\">the concept of agent convergence</a> a while back and it's still one of my favorites. Worth the read if you missed it!</p>\n</blockquote>\n\n\n<hr>\n\n<h2>\n  <a name=\"add-another-layer-of-abstraction\" href=\"#add-another-layer-of-abstraction\">\n  </a>\n  Add Another Layer of Abstraction ğŸªœ\n</h2>\n\n<p>Now for my version of truth, which I know a lot of you are going to hate so go ahead and brace for it. Once youâ€™re working in a smart orchestration-driven flow, there's no reason you need to keep prompting from the IDE. Wait before you jump into the debate, thoughâ€”I'm not saying the IDE becomes obsolete! It just stops being the primary interface for developer workflows because youâ€™re consistently able to work at a higher level of abstraction. In this future, developers are directing systems that generate, test, and validate the code several layers underneath you automatically.</p>\n\n<p>Youâ€™re orchestrating agents that direct other agents. Some run sequentially. Others will run in parallel. Documentation is generated automatically and added to the agent's working knowledge base. Tests run continuouslyÂ alongside agents implementing new code. Integration testing matters. Systems testing matters more. Chaos testing morphs from an abstract concept into a baseline requirement. The code still existsâ€”but itâ€™s no longer written by or for humans. AI slowly takes that over, which makes natural language the newest language you need to learn.</p>\n\n<blockquote>\n<p>ğŸ¦„ For the record, developers are most definitely still building and driving solutions. That will never changeâ€”we're the mad scientists thinking up wild potions you didn't know you needed! Besides, all the future advancements in the world won't give silicon the ability to invent new things. Humans create. AI helps. <em>Period</em>.</p>\n</blockquote>\n\n\n<hr>\n\n<h2>\n  <a name=\"trust-then-speed-not-the-other-way-around\" href=\"#trust-then-speed-not-the-other-way-around\">\n  </a>\n  Trust, Then Speed (not the other way around) ğŸï¸\n</h2>\n\n<p>When something breaks in any of my workflows, I donâ€™t correct the mistake in the code immediately. I start by correcting whatever instruction caused the mistake, and then I rerun it. Even when Iâ€™m busy, even when work is chaotic, and especially when I should have left it alone hours agoâ€”I never fully disengage from this. <em>I canâ€™t.</em></p>\n\n<p>This is exactly why AI doesnâ€™t make you fasterâ€”not yet, anyway. Not because it canâ€™t, but because the systems havenâ€™t caught up to where speed actually emerges. If youâ€™re learning to use AI correctly, it almost always makes you slower at firstâ€”not faster. The delay isnâ€™t failure. Itâ€™s infrastructure lag.</p>\n\n<p>Think of it like an investment. Youâ€™re learning how the models behave and how instructions actually align with them. Youâ€™re learning where the limits are, and then deliberately making the system work within those constraints. Speed comes laterâ€”after you trust that the system returns results that are validated, reviewed, and tested because you built it to behave that way.</p>\n\n<p>AI evolution is a long game, and weâ€™re barely getting started. Right now, it still feels like grade school. Weâ€™re teaching it what our world looks like, how we think, and where the boundaries are.</p>\n\n<p>All the work done nowâ€”in this awkward middle stateâ€”is what makes that learning possible. Long runs of trial-and-error prompts, walls of instructions, documentation that later turns into knowledge basesâ€”thatâ€™s the curriculum. And by the time itâ€™s ready to graduate, it wonâ€™t just be competent. Itâ€™ll be a master. Thatâ€™s the moment you realize you trust AIâ€”not because itâ€™s autonomous, but because you finally are. ğŸ‰ğŸ§šâ€â™€ï¸</p>\n\n\n<hr>\n\n<h2>\n  <a name=\"i-worked-until-it-worked\" href=\"#i-worked-until-it-worked\">\n  </a>\n  ğŸ›¡ï¸ I Worked Until It Worked\n</h2>\n\n<p>This post was written by me, with ChatGPT nearby like an overly talkative whiteboardâ€”listening, interrupting, getting corrected, and occasionally making a genuinely good point. We argued about structure, laughed at the mic cutting out at the worst moments, and kept going anyway. The opinions are mine. The fact that it finally worked is the point.</p>\n\n",
    "description": "Why AI orchestration, context limits, and trust matter more than speedâ€”and what building for the next five to ten years actually looks like.",
    "slug": "waiting-with-intent-designing-ai-systems-for-the-long-game-1abg",
    "cover_image": "https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frofa5lb8e99t19vma01h.png",
    "tags": [
      "agentic",
      "ai",
      "architecture",
      "devrel"
    ],
    "author": "Ashley Childress",
    "api_data": {
      "type_of": "article",
      "id": 3162975,
      "title": "Waiting, With Intent: Designing AI Systems for the Long Game ğŸ§­",
      "description": "Why AI orchestration, context limits, and trust matter more than speedâ€”and what building for the next five to ten years actually looks like.",
      "readable_publish_date": "Jan 14",
      "slug": "waiting-with-intent-designing-ai-systems-for-the-long-game-1abg",
      "path": "/anchildress1/waiting-with-intent-designing-ai-systems-for-the-long-game-1abg",
      "url": "https://dev.to/anchildress1/waiting-with-intent-designing-ai-systems-for-the-long-game-1abg",
      "comments_count": 0,
      "public_reactions_count": 0,
      "collection_id": null,
      "published_timestamp": "2026-01-14T12:22:00Z",
      "language": "en",
      "subforem_id": 1,
      "positive_reactions_count": 0,
      "cover_image": "https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frofa5lb8e99t19vma01h.png",
      "social_image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frofa5lb8e99t19vma01h.png",
      "canonical_url": "https://dev.to/anchildress1/waiting-with-intent-designing-ai-systems-for-the-long-game-1abg",
      "created_at": "2026-01-10T16:00:35Z",
      "edited_at": "2026-01-12T23:25:44Z",
      "crossposted_at": null,
      "published_at": "2026-01-14T12:22:00Z",
      "last_comment_at": "2026-01-14T13:22:00Z",
      "reading_time_minutes": 8,
      "tag_list": "agentic, ai, architecture, devrel",
      "tags": [
        "agentic",
        "ai",
        "architecture",
        "devrel"
      ],
      "body_html": "<blockquote>\n<p>ğŸ¦„ Iâ€™m waiting for AI to mature. Very explicitlyâ€”and yes, mostly impatiently. I donâ€™t even think we're close to imagining the future landscape with AI, and honestly pretending otherwise is neither honest or useful to anyone. This post is my attempt to explain how I think about AI from a dev perspective on a longer horizonâ€”five, maybe even ten years down the road. The tools we have right now are still a very long way away from my baseline expectations, which my AI systems remind me of near constantlyâ€”like when I'm trying to force agent-like functionality out of ChatGPT. <strong>Spoiler:</strong> itâ€™s not designed to handle that.</p>\n\n<p>While Iâ€™m waiting, though, Iâ€™m not disengaged. Iâ€™m definitely tinkeringâ€”sometimes randomly and sometimes just as an unsatisfied AI user whoâ€™s not thrilled with the existing systems. Iâ€™m also busy figuring out what the next problems really look like by diving in and getting my hands dirty. </p>\n\n<p>One of those big challenges is what I keep calling the â€œmemory problem.â€ I've designed a solution for my own personal agent to manage long-term memory. Yesâ€”I'm aware that GitHub is inevitably going to beat me to a viable solution. <em>Again</em>. But I'm one of those people who will attempt to solve a problem first, get it wrong at least ten different times, and <em>then</em> do the research to fill in the knowledge gaps. Now I just have to muster up enough oomph to actually do it. ğŸ‰ğŸ§šâ€â™€ï¸</p>\n</blockquote>\n\n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg4wzwazhknufn2ou2to7.png\" class=\"article-body-image-wrapper\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg4wzwazhknufn2ou2to7.png\" alt=\"Human-crafted, AI-edited badge\" loading=\"lazy\" width=\"200\" height=\"200\"></a></p>\n\n\n<hr>\n\n<h2>\n  <a name=\"first-principles-llm-vs-agent%C2%A0\" href=\"#first-principles-llm-vs-agent%C2%A0\">\n  </a>\n  First Principles: LLM vs AgentÂ ğŸ§©\n</h2>\n\n<p>At some point, if you want any of this AI talk to make sense, you have to step back, align terminology, and separate concepts that keep getting blurred together. An LLM, often called a model, is the generative part of GenAIâ€”it accepts input and generates output. <em>That's it.</em> An agent is the system managing context, memory, and various tools. The agent is responsible for what information the LLM even sees in the first place.</p>\n\n<p>When those two ideas get collapsed into the same thing, everything downstream becomes confused. You canâ€™t reason clearly about limits, costs, or failure modes if you donâ€™t separate generation from data management. Until you draw that line, every other discussion ends up muddy.</p>\n\n\n<hr>\n\n<h2>\n  <a name=\"context-is-the-bottleneck-and-everyone-knows-it%C2%A0\" href=\"#context-is-the-bottleneck-and-everyone-knows-it%C2%A0\">\n  </a>\n  Context Is the Bottleneck (and Everyone Knows It)Â ğŸ•¸ï¸\n</h2>\n\n<p>Once you make the distinction between LLM and agent, the real bottleneck becomes obvious. There is no good way to manage context today, let alone have the agent automate that job effectively. If youâ€™re not fully up to date on the lingo: context includes a whole set of things like instruction files, workspace structure, active files in your IDE, the AI chat history, available tools, and more.</p>\n\n<p>What we have now are very manual tools that do very little to solve the problem. We have to remember to tell the AI which parts currently matterâ€”or at some point we have to clear the chat entirely and start over. If we donâ€™t do that deliberately, AI slowly loses the point of whatâ€™s we're supposed to be working on in the first place. At worst, the entire chat thread is poisoned and the AI becomes unable to function at all. Then you're forced to start fresh and always at the most inconvenient time.</p>\n\n<p>And donâ€™t expect LLM context to scale, either. Hardware costs may go down eventually, but nowhere near fast enough to keep up with everything we keep throwing at it. So, context is very finiteâ€”especially in GitHub where context windows are smaller than normal anyway.</p>\n\n<p>The agent will typically make space by compacting information. ItÂ will ask the LLM to summarize key points and then it literally drops the original full length novel completely from your active context and replaces it with the cliff notes version. The more summarization, the less accurate things get over time. So naturally you retry prompts while adding back the dropped details and you end up making more calls for a single task overall. The model has to process more and more input just to get you back to the same answer you already hadâ€”not necessarily a better one.</p>\n\n<p>People know this is a problem. Tools like <a href=\"https://github.com/toon-format/toon\" target=\"_blank\" rel=\"noopener noreferrer\">Toon</a> exist specifically to minimize input impact for AI. We also have tools like <a href=\"https://docs.github.com/?search-overlay-open=true&amp;search-overlay-input=runSubagent&amp;search-overlay-ask-ai=true\" target=\"_blank\" rel=\"noopener noreferrer\">Copilot's <code>#runSubagent</code></a> to help manage context within a single agent. These aren't true solutions thoughâ€”they are signals. These are the problems people are trying to solve yesterday while we wait for the next AI evolution to emerge.</p>\n\n\n<hr>\n\n<h2>\n  <a name=\"why-orchestration-is-inevitable%C2%A0\" href=\"#why-orchestration-is-inevitable%C2%A0\">\n  </a>\n  Why Orchestration Is InevitableÂ ğŸ™\n</h2>\n\n<p>Even if you do everything â€œrightâ€ and manage context like a master AI sensei, agents eventually hit a limit. The list of must-have MCPs is growing and right now those stay in the context window as long as they're enabled. Projects are starting and accumulating larger knowledge bases. Customization is becoming more and more explicit. The context an agent needs to use will continue to grow exponentially, even though LLMs aren't increasing capacity at the same speed.</p>\n\n<p>The ultimate overflow state isnâ€™t hypotheticalâ€”itâ€™s inevitable. Once an agent accumulates enough memory, enough history, enough summarization, the LLM simply canâ€™t keep up coherently anymore. That isnâ€™t a failure in the systemâ€”itâ€™s a limit.</p>\n\n<p>When you hit that limit, you can't just tweak prompts or optimize harder. You wouldn't try to squeeze more juice out of the same dry orange, either. The only real long-term solution is that you split the systemâ€”<em>you have to</em>!</p>\n\n<p>Smaller pieces of work are then sent to the LLM with only relevant context, which is when smarter agents will start to appear. This is where summarization stops and you retain the original intent at both a high-level and at the lowest-level. When we get here, AI generation stops being the problemâ€”the new problem is coordinating all those tiny pieces of work and still accomplishing the larger goal without re-prompting anything previously stated or defined elsewhere already. <em>Welcome to the world of true agent orchestration</em>!</p>\n\n<blockquote>\n<p>ğŸ’¡ <strong>ProTip:</strong> If you want a sneak peek of what this looks like, check out <a href=\"https://verdent.ai\" target=\"_blank\" rel=\"noopener noreferrer\">Verdent.ai</a>. Of all the solutions I've worked with, Verdent is the only one that's truly designed for agent orchestration. It also excels in VS Code and wins every coding competition I've put it in.</p>\n</blockquote>\n\n\n<hr>\n\n<h2>\n  <a name=\"orchestration-as-a-system-property\" href=\"#orchestration-as-a-system-property\">\n  </a>\n  Orchestration as a System Property â™Ÿï¸\n</h2>\n\n<p>Orchestration isnâ€™t just about sequencing work in a nicer wayâ€”itâ€™s about changing where responsibility lives. Yesâ€”some things are always going to be sequential, but not everything needs to be. Some things can and should run in parallel, especially if you want speed and reliability included in future agentic systems.</p>\n\n<p>Validation is a fundamental part of orchestration, not something bolted on afterward. A successful agent has to be able to verify its own work without relying on prior context. It has to come in like a third party, with no knowledge beyond the repo instructions. CodeQL, lint enforcement, Makefiles, and even extra tests become the ground truth the system must consistently check itself against.</p>\n\n<p>Multi-model opposition fits naturally here, too. Different models trained by different companies catch different things. Then the agent can pick one modelÂ  to implement and another to review. The point is that they disagree by default and then they converge around a common goal. This is a pivotal moment in the future landscape becauseÂ officially the LLM is no longer the center of gravityâ€”the agentic system is.</p>\n\n<blockquote>\n<p>ğŸ¤ <strong>ShoutOut</strong> <a class=\"mentioned-user\" href=\"https://dev.to/marcosomma\">@marcosomma</a> wrote a brilliant article on <a href=\"https://dev.to/marcosomma/loopnode-how-orka-orchestrates-iterated-thought-until-agreement-emerges-17l2\">the concept of agent convergence</a> a while back and it's still one of my favorites. Worth the read if you missed it!</p>\n</blockquote>\n\n\n<hr>\n\n<h2>\n  <a name=\"add-another-layer-of-abstraction\" href=\"#add-another-layer-of-abstraction\">\n  </a>\n  Add Another Layer of Abstraction ğŸªœ\n</h2>\n\n<p>Now for my version of truth, which I know a lot of you are going to hate so go ahead and brace for it. Once youâ€™re working in a smart orchestration-driven flow, there's no reason you need to keep prompting from the IDE. Wait before you jump into the debate, thoughâ€”I'm not saying the IDE becomes obsolete! It just stops being the primary interface for developer workflows because youâ€™re consistently able to work at a higher level of abstraction. In this future, developers are directing systems that generate, test, and validate the code several layers underneath you automatically.</p>\n\n<p>Youâ€™re orchestrating agents that direct other agents. Some run sequentially. Others will run in parallel. Documentation is generated automatically and added to the agent's working knowledge base. Tests run continuouslyÂ alongside agents implementing new code. Integration testing matters. Systems testing matters more. Chaos testing morphs from an abstract concept into a baseline requirement. The code still existsâ€”but itâ€™s no longer written by or for humans. AI slowly takes that over, which makes natural language the newest language you need to learn.</p>\n\n<blockquote>\n<p>ğŸ¦„ For the record, developers are most definitely still building and driving solutions. That will never changeâ€”we're the mad scientists thinking up wild potions you didn't know you needed! Besides, all the future advancements in the world won't give silicon the ability to invent new things. Humans create. AI helps. <em>Period</em>.</p>\n</blockquote>\n\n\n<hr>\n\n<h2>\n  <a name=\"trust-then-speed-not-the-other-way-around\" href=\"#trust-then-speed-not-the-other-way-around\">\n  </a>\n  Trust, Then Speed (not the other way around) ğŸï¸\n</h2>\n\n<p>When something breaks in any of my workflows, I donâ€™t correct the mistake in the code immediately. I start by correcting whatever instruction caused the mistake, and then I rerun it. Even when Iâ€™m busy, even when work is chaotic, and especially when I should have left it alone hours agoâ€”I never fully disengage from this. <em>I canâ€™t.</em></p>\n\n<p>This is exactly why AI doesnâ€™t make you fasterâ€”not yet, anyway. Not because it canâ€™t, but because the systems havenâ€™t caught up to where speed actually emerges. If youâ€™re learning to use AI correctly, it almost always makes you slower at firstâ€”not faster. The delay isnâ€™t failure. Itâ€™s infrastructure lag.</p>\n\n<p>Think of it like an investment. Youâ€™re learning how the models behave and how instructions actually align with them. Youâ€™re learning where the limits are, and then deliberately making the system work within those constraints. Speed comes laterâ€”after you trust that the system returns results that are validated, reviewed, and tested because you built it to behave that way.</p>\n\n<p>AI evolution is a long game, and weâ€™re barely getting started. Right now, it still feels like grade school. Weâ€™re teaching it what our world looks like, how we think, and where the boundaries are.</p>\n\n<p>All the work done nowâ€”in this awkward middle stateâ€”is what makes that learning possible. Long runs of trial-and-error prompts, walls of instructions, documentation that later turns into knowledge basesâ€”thatâ€™s the curriculum. And by the time itâ€™s ready to graduate, it wonâ€™t just be competent. Itâ€™ll be a master. Thatâ€™s the moment you realize you trust AIâ€”not because itâ€™s autonomous, but because you finally are. ğŸ‰ğŸ§šâ€â™€ï¸</p>\n\n\n<hr>\n\n<h2>\n  <a name=\"i-worked-until-it-worked\" href=\"#i-worked-until-it-worked\">\n  </a>\n  ğŸ›¡ï¸ I Worked Until It Worked\n</h2>\n\n<p>This post was written by me, with ChatGPT nearby like an overly talkative whiteboardâ€”listening, interrupting, getting corrected, and occasionally making a genuinely good point. We argued about structure, laughed at the mic cutting out at the worst moments, and kept going anyway. The opinions are mine. The fact that it finally worked is the point.</p>\n\n",
      "body_markdown": "---\ntitle: Waiting, With Intent: Designing AI Systems for the Long Game ğŸ§­\npublished: true\ndescription: Why AI orchestration, context limits, and trust matter more than speedâ€”and what building for the next five to ten years actually looks like.\ntags: [agentic, ai, architecture, devrel]\ncover_image: https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rofa5lb8e99t19vma01h.png\n# Use a ratio of 100:42 for best results.\npublished_at: 2026-01-14 07:22 -0500\n---\n\n> ğŸ¦„ Iâ€™m waiting for AI to mature. Very explicitlyâ€”and yes, mostly impatiently. I donâ€™t even think we're close to imagining the future landscape with AI, and honestly pretending otherwise is neither honest or useful to anyone. This post is my attempt to explain how I think about AI from a dev perspective on a longer horizonâ€”five, maybe even ten years down the road. The tools we have right now are still a very long way away from my baseline expectations, which my AI systems remind me of near constantlyâ€”like when I'm trying to force agent-like functionality out of ChatGPT. **Spoiler:** itâ€™s not designed to handle that.\n>\n> While Iâ€™m waiting, though, Iâ€™m not disengaged. Iâ€™m definitely tinkeringâ€”sometimes randomly and sometimes just as an unsatisfied AI user whoâ€™s not thrilled with the existing systems. Iâ€™m also busy figuring out what the next problems really look like by diving in and getting my hands dirty. \n> \n> One of those big challenges is what I keep calling the â€œmemory problem.â€ I've designed a solution for my own personal agent to manage long-term memory. Yesâ€”I'm aware that GitHub is inevitably going to beat me to a viable solution. _Again_. But I'm one of those people who will attempt to solve a problem first, get it wrong at least ten different times, and _then_ do the research to fill in the knowledge gaps. Now I just have to muster up enough oomph to actually do it. ğŸ‰ğŸ§šâ€â™€ï¸\n\n![Human-crafted, AI-edited badge](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g4wzwazhknufn2ou2to7.png)\n\n---\n\n## First Principles: LLM vs AgentÂ ğŸ§©\n\nAt some point, if you want any of this AI talk to make sense, you have to step back, align terminology, and separate concepts that keep getting blurred together. An LLM, often called a model, is the generative part of GenAIâ€”it accepts input and generates output. _That's it._ An agent is the system managing context, memory, and various tools. The agent is responsible for what information the LLM even sees in the first place.\n\nWhen those two ideas get collapsed into the same thing, everything downstream becomes confused. You canâ€™t reason clearly about limits, costs, or failure modes if you donâ€™t separate generation from data management. Until you draw that line, every other discussion ends up muddy.\n\n---\n\n## Context Is the Bottleneck (and Everyone Knows It)Â ğŸ•¸ï¸\n\nOnce you make the distinction between LLM and agent, the real bottleneck becomes obvious. There is no good way to manage context today, let alone have the agent automate that job effectively. If youâ€™re not fully up to date on the lingo: context includes a whole set of things like instruction files, workspace structure, active files in your IDE, the AI chat history, available tools, and more.\n\nWhat we have now are very manual tools that do very little to solve the problem. We have to remember to tell the AI which parts currently matterâ€”or at some point we have to clear the chat entirely and start over. If we donâ€™t do that deliberately, AI slowly loses the point of whatâ€™s we're supposed to be working on in the first place. At worst, the entire chat thread is poisoned and the AI becomes unable to function at all. Then you're forced to start fresh and always at the most inconvenient time.\n\nAnd donâ€™t expect LLM context to scale, either. Hardware costs may go down eventually, but nowhere near fast enough to keep up with everything we keep throwing at it. So, context is very finiteâ€”especially in GitHub where context windows are smaller than normal anyway.\n\nThe agent will typically make space by compacting information. ItÂ will ask the LLM to summarize key points and then it literally drops the original full length novel completely from your active context and replaces it with the cliff notes version. The more summarization, the less accurate things get over time. So naturally you retry prompts while adding back the dropped details and you end up making more calls for a single task overall. The model has to process more and more input just to get you back to the same answer you already hadâ€”not necessarily a better one.\n\nPeople know this is a problem. Tools like [Toon](https://github.com/toon-format/toon) exist specifically to minimize input impact for AI. We also have tools like [Copilot's `#runSubagent`](https://docs.github.com/?search-overlay-open=true&search-overlay-input=runSubagent&search-overlay-ask-ai=true) to help manage context within a single agent. These aren't true solutions thoughâ€”they are signals. These are the problems people are trying to solve yesterday while we wait for the next AI evolution to emerge.\n\n---\n\n## Why Orchestration Is InevitableÂ ğŸ™\n\nEven if you do everything â€œrightâ€ and manage context like a master AI sensei, agents eventually hit a limit. The list of must-have MCPs is growing and right now those stay in the context window as long as they're enabled. Projects are starting and accumulating larger knowledge bases. Customization is becoming more and more explicit. The context an agent needs to use will continue to grow exponentially, even though LLMs aren't increasing capacity at the same speed.\n\nThe ultimate overflow state isnâ€™t hypotheticalâ€”itâ€™s inevitable. Once an agent accumulates enough memory, enough history, enough summarization, the LLM simply canâ€™t keep up coherently anymore. That isnâ€™t a failure in the systemâ€”itâ€™s a limit.\n\nWhen you hit that limit, you can't just tweak prompts or optimize harder. You wouldn't try to squeeze more juice out of the same dry orange, either. The only real long-term solution is that you split the systemâ€”_you have to_!\n\nSmaller pieces of work are then sent to the LLM with only relevant context, which is when smarter agents will start to appear. This is where summarization stops and you retain the original intent at both a high-level and at the lowest-level. When we get here, AI generation stops being the problemâ€”the new problem is coordinating all those tiny pieces of work and still accomplishing the larger goal without re-prompting anything previously stated or defined elsewhere already. _Welcome to the world of true agent orchestration_!\n\n> ğŸ’¡ **ProTip:** If you want a sneak peek of what this looks like, check out [Verdent.ai](https://verdent.ai). Of all the solutions I've worked with, Verdent is the only one that's truly designed for agent orchestration. It also excels in VS Code and wins every coding competition I've put it in.\n\n---\n\n## Orchestration as a System Property â™Ÿï¸\n\nOrchestration isnâ€™t just about sequencing work in a nicer wayâ€”itâ€™s about changing where responsibility lives. Yesâ€”some things are always going to be sequential, but not everything needs to be. Some things can and should run in parallel, especially if you want speed and reliability included in future agentic systems.\n\nValidation is a fundamental part of orchestration, not something bolted on afterward. A successful agent has to be able to verify its own work without relying on prior context. It has to come in like a third party, with no knowledge beyond the repo instructions. CodeQL, lint enforcement, Makefiles, and even extra tests become the ground truth the system must consistently check itself against.\n\nMulti-model opposition fits naturally here, too. Different models trained by different companies catch different things. Then the agent can pick one modelÂ  to implement and another to review. The point is that they disagree by default and then they converge around a common goal. This is a pivotal moment in the future landscape becauseÂ officially the LLM is no longer the center of gravityâ€”the agentic system is.\n\n> ğŸ¤ **ShoutOut** @marcosomma wrote a brilliant article on [the concept of agent convergence](https://dev.to/marcosomma/loopnode-how-orka-orchestrates-iterated-thought-until-agreement-emerges-17l2) a while back and it's still one of my favorites. Worth the read if you missed it!\n\n---\n\n## Add Another Layer of Abstraction ğŸªœ\n\nNow for my version of truth, which I know a lot of you are going to hate so go ahead and brace for it. Once youâ€™re working in a smart orchestration-driven flow, there's no reason you need to keep prompting from the IDE. Wait before you jump into the debate, thoughâ€”I'm not saying the IDE becomes obsolete! It just stops being the primary interface for developer workflows because youâ€™re consistently able to work at a higher level of abstraction. In this future, developers are directing systems that generate, test, and validate the code several layers underneath you automatically.\n\nYouâ€™re orchestrating agents that direct other agents. Some run sequentially. Others will run in parallel. Documentation is generated automatically and added to the agent's working knowledge base. Tests run continuouslyÂ alongside agents implementing new code. Integration testing matters. Systems testing matters more. Chaos testing morphs from an abstract concept into a baseline requirement. The code still existsâ€”but itâ€™s no longer written by or for humans. AI slowly takes that over, which makes natural language the newest language you need to learn.\n\n> ğŸ¦„ For the record, developers are most definitely still building and driving solutions. That will never changeâ€”we're the mad scientists thinking up wild potions you didn't know you needed! Besides, all the future advancements in the world won't give silicon the ability to invent new things. Humans create. AI helps. _Period_.\n\n---\n\n## Trust, Then Speed (not the other way around) ğŸï¸\n\nWhen something breaks in any of my workflows, I donâ€™t correct the mistake in the code immediately. I start by correcting whatever instruction caused the mistake, and then I rerun it. Even when Iâ€™m busy, even when work is chaotic, and especially when I should have left it alone hours agoâ€”I never fully disengage from this. _I canâ€™t._\n\nThis is exactly why AI doesnâ€™t make you fasterâ€”not yet, anyway. Not because it canâ€™t, but because the systems havenâ€™t caught up to where speed actually emerges. If youâ€™re learning to use AI correctly, it almost always makes you slower at firstâ€”not faster. The delay isnâ€™t failure. Itâ€™s infrastructure lag.\n\nThink of it like an investment. Youâ€™re learning how the models behave and how instructions actually align with them. Youâ€™re learning where the limits are, and then deliberately making the system work within those constraints. Speed comes laterâ€”after you trust that the system returns results that are validated, reviewed, and tested because you built it to behave that way.\n\nAI evolution is a long game, and weâ€™re barely getting started. Right now, it still feels like grade school. Weâ€™re teaching it what our world looks like, how we think, and where the boundaries are.\n\nAll the work done nowâ€”in this awkward middle stateâ€”is what makes that learning possible. Long runs of trial-and-error prompts, walls of instructions, documentation that later turns into knowledge basesâ€”thatâ€™s the curriculum. And by the time itâ€™s ready to graduate, it wonâ€™t just be competent. Itâ€™ll be a master. Thatâ€™s the moment you realize you trust AIâ€”not because itâ€™s autonomous, but because you finally are. ğŸ‰ğŸ§šâ€â™€ï¸\n\n---\n\n## ğŸ›¡ï¸ I Worked Until It Worked\n\nThis post was written by me, with ChatGPT nearby like an overly talkative whiteboardâ€”listening, interrupting, getting corrected, and occasionally making a genuinely good point. We argued about structure, laughed at the mic cutting out at the worst moments, and kept going anyway. The opinions are mine. The fact that it finally worked is the point.",
      "user": {
        "name": "Ashley Childress",
        "username": "anchildress1",
        "twitter_username": null,
        "github_username": "anchildress1",
        "user_id": 3224358,
        "website_url": "http://www.linkedin.com/in/anchildress1",
        "profile_image": "https://media2.dev.to/dynamic/image/width=640,height=640,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F3224358%2F7f675c78-6aa0-466a-a5a7-c3e35440d53a.png",
        "profile_image_90": "https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F3224358%2F7f675c78-6aa0-466a-a5a7-c3e35440d53a.png"
      }
    }
  }
]