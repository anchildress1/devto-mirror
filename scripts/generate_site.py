import os, pathlib, re, html, json, requests
from datetime import datetime, timezone
from email.utils import parsedate_to_datetime
from slugify import slugify
from jinja2 import Template
from utils import INDEX_TMPL, SITEMAP_TMPL, parse_date, dedupe_posts_by_link

DEVTO_USERNAME = os.getenv("DEVTO_USERNAME", "").strip()
PAGES_REPO = os.getenv("PAGES_REPO", "").strip()  # "user/repo"
LAST_RUN_FILE = "last_run.txt"

assert DEVTO_USERNAME, "Missing DEVTO_USERNAME (your Dev.to username)"
assert "/" in PAGES_REPO, "Invalid PAGES_REPO (expected 'user/repo')"

username, repo = PAGES_REPO.split("/")
HOME = f"https://{username}.github.io/{repo}/"

ROOT = pathlib.Path(".")
POSTS_DIR = ROOT / "posts"
POSTS_DIR.mkdir(parents=True, exist_ok=True)

def get_last_run_timestamp():
    """Reads the timestamp from the last successful run."""
    p = pathlib.Path(LAST_RUN_FILE)
    if not p.exists():
        return None
    return p.read_text(encoding="utf-8").strip()

def set_last_run_timestamp():
    """Writes the current UTC timestamp to the run file."""
    p = pathlib.Path(LAST_RUN_FILE)
    p.write_text(datetime.now(timezone.utc).isoformat(), encoding="utf-8")

def fetch_all_articles_from_api(last_run_iso=None):
    """Fetch all articles from the Dev.to API, paginating if necessary.
    If last_run_iso is provided, it will only fetch articles published after that time.
    """
    articles = []
    page = 1
    while True:
        print(f"Fetching page {page} of articles...")
        api_url = f"https://dev.to/api/articles?username={DEVTO_USERNAME}&page={page}&per_page=100"
        response = requests.get(api_url)
        response.raise_for_status()
        data = response.json()
        if not data:
            break

        new_articles = data
        # If a last run timestamp is provided, filter out older posts
        if last_run_iso:
            last_run_dt = datetime.fromisoformat(last_run_iso)
            new_articles = [
                article for article in data
                if datetime.fromisoformat(article['published_at'].replace("Z", "+00:00")) > last_run_dt
            ]

        articles.extend(new_articles)

        # If we are filtering and the number of articles fetched is less than a full page,
        # or if we added fewer articles than we fetched, we can stop.
        if last_run_iso and (len(data) < 100 or len(new_articles) < len(data)):
             break

        page += 1

    print(f"Found {len(articles)} new or updated articles since last run.")
    return articles


# ----------------------------
# Templates (posts + index)
# ----------------------------
PAGE_TMPL = Template("""<!doctype html><html lang="en"><head>
<meta charset="utf-8">
<title>{{ title }}</title>
<link rel="canonical" href="{{ canonical }}">
<meta name="description" content="{{ description }}">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head><body>
<main>
  <h1><a href="{{ canonical }}">{{ title }}</a></h1>
  {% if date %}<p><em>Published: {{ date }}</em></p>{% endif %}
  <article>{{ content }}</article>
  <p><a href="{{ canonical }}">Read on Dev.to →</a></p>
</main>
</body></html>
""")

COMMENT_NOTE_TMPL = Template("""<!doctype html><html lang="en"><head>
<meta charset="utf-8">
<title>{{ title }}</title>
<link rel="canonical" href="{{ canonical }}">
<meta name="description" content="{{ description }}">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head><body>
<main>
  <h1>{{ title }}</h1>
  {% if context %}<p>{{ context }}</p>{% endif %}
  <p><a href="{{ url }}">Open on Dev.to →</a></p>
</main>
</body></html>
""")

# ----------------------------
# robots + sitemap
# ----------------------------
ROBOTS_TMPL = """# Comprehensive robots.txt generated by scripts/generate_site.py
# This file allows a broad set of search engines, AI crawlers, archivers, SEO tools,
# and social preview agents. Edit as needed.

User-agent: *
Allow: /

# Major search engines
User-agent: Googlebot
Allow: /

User-agent: Google-Extended
Allow: /

User-agent: Bingbot
Allow: /

User-agent: DuckDuckBot
Allow: /

User-agent: Baiduspider
Allow: /

User-agent: Yandex
Allow: /

User-agent: Sogou
Allow: /

User-agent: Exabot
Allow: /

# AI and assistant crawlers
User-agent: GPTBot
Allow: /

User-agent: ClaudeBot
Allow: /

User-agent: Claude-Web
Allow: /

User-agent: PerplexityBot
Allow: /

User-agent: Bytespider
Allow: /

User-agent: HuggingFace
Allow: /

User-agent: BingPreview
Allow: /

# SEO, link research, and site-auditing bots
User-agent: AhrefsBot
Allow: /

User-agent: SemrushBot
Allow: /

User-agent: MJ12bot
Allow: /

User-agent: DotBot
Allow: /

# Archival crawlers
User-agent: CCBot
Allow: /

User-agent: ia_archiver
Allow: /

User-agent: archive.org_bot
Allow: /

# Social preview and embed bots
User-agent: facebookexternalhit
Allow: /

User-agent: Facebot
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: Slackbot
Allow: /

User-agent: LinkedInBot
Allow: /

User-agent: Pinterestbot
Allow: /

# Misc / vendor-specific
User-agent: Applebot
Allow: /

User-agent: Sistrix
Allow: /

User-agent: OpenWebSpider
Allow: /

# Default fallback (already covered above but kept for clarity)
User-agent: *
Allow: /

Sitemap: {home}sitemap.xml
"""

# ----------------------------
# Helpers
# ----------------------------
def strip_html(text):
    # Remove HTML tags, collapse internal whitespace (newlines/tabs/extra spaces)
    # into single spaces, then trim leading/trailing whitespace.
    if not text:
        return ""
    no_tags = re.sub(r"<[^>]+>", "", text)
    collapsed = re.sub(r"\s+", " ", no_tags)
    return collapsed.strip()

class Post:
    def __init__(self, api_data):
        self.title = api_data.get("title", "Untitled")
        self.link = api_data.get("url", HOME)
        self.date = api_data.get("published_at", "")
        self.content_html = api_data.get("body_html", "")

        # Use the API's description, truncate for SEO
        desc_text = (api_data.get("description", "") or "").strip()
        self.description = desc_text[:160]

        # Use the slug directly from the API
        self.slug = api_data.get("slug", slugify(self.title) or "post")

    def to_dict(self):
        """Convert Post to dictionary for JSON serialization"""
        # Ensure date is a string (ISO if possible)
        date_val = self.date
        if isinstance(self.date, datetime):
            date_val = self.date.isoformat()
        return {
            'title': self.title,
            'link': self.link,
            'date': date_val,
            'content_html': self.content_html,
            'description': self.description,
            'slug': self.slug
        }

    @classmethod
    def from_dict(cls, data):
        """Create Post from dictionary loaded from JSON"""
        post = cls.__new__(cls)
        post.title = data['title']
        post.link = data['link']
        post.date = data['date']
        post.content_html = data['content_html']
        post.description = data['description']
        post.slug = data['slug']
        return post

def load_comment_manifest(path="comments.txt"):
    """Read lines of: URL | optional context (one line)."""
    items = []
    p = pathlib.Path(path)
    if not p.exists():
        return items
    for raw in p.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        url, *ctx = [s.strip() for s in line.split("|", 1)]
        context = ctx[0] if ctx else ""
        # get a stable id from /comment/<id> or #comment-<id>, else slug of URL
        m = re.search(r"/comment/([A-Za-z0-9]+)", url) or re.search(r"#comment-([A-Za-z0-9_-]+)", url)
        cid = m.group(1) if m else slugify(url)[:48]
        local = f"comments/{cid}.html"
        # short label for index
        label = (context or url)
        if len(label) > 80:
            label = label[:77] + "..."
        items.append({"url": url, "context": context, "local": local, "text": label})
    return items

def load_existing_posts(path="posts_data.json"):
    """Load existing posts from JSON file"""
    p = pathlib.Path(path)
    if not p.exists():
        return []
    try:
        with open(p, 'r', encoding='utf-8') as f:
            posts_data = json.load(f)
            # Convert dicts to Post instances (avoid re-parsing RSS entries)
            return [Post.from_dict(post_dict) for post_dict in posts_data]
    except (json.JSONDecodeError, KeyError):
        return []


def _parse_date_str(datestr):
    """Try to parse a date string from RFC or ISO formats. Returns a timezone-aware
    datetime when possible, otherwise None."""
    return parse_date(datestr)

def save_posts_data(posts, path="posts_data.json"):
    """Save posts to JSON file"""
    posts_data = [post.to_dict() for post in posts]
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(posts_data, f, indent=2, ensure_ascii=False)

def is_first_run():
    """Check if this is the first run by looking for posts_data.json"""
    return not pathlib.Path("posts_data.json").exists()

def find_new_posts(api_articles, existing_posts):
    """Find new posts from the API that aren't in existing posts."""
    existing_links = {post['link'] for post in existing_posts}
    new_posts = [
        Post(article) for article in api_articles
        if article['url'] not in existing_links
    ]
    return new_posts

# ----------------------------
# Build posts (incremental updates)
# ----------------------------
last_run_timestamp = get_last_run_timestamp()
api_articles = fetch_all_articles_from_api(last_run_timestamp)

# Convert API articles to Post objects
new_posts = [Post(article) for article in api_articles]

if not new_posts:
    print("No new posts to process. Exiting.")
    # Still update the timestamp to avoid re-checking the same period
    set_last_run_timestamp()
    exit()

# Load existing posts from previous runs
existing_posts_data = load_existing_posts()

# Create a set of existing post URLs for faster lookup
existing_links = {p.get('link', '') for p in existing_posts_data}

# Filter out new posts that already exist
truly_new_posts = []
for post in new_posts:
    if post.link not in existing_links:
        truly_new_posts.append(post)

# Combine all posts: existing + truly new
all_posts_data = existing_posts_data.copy()
for post in truly_new_posts:
    all_posts_data.append(post.to_dict())

# Deduplicate and sort all posts by date, newest first
all_posts_data = dedupe_posts_by_link(all_posts_data)

# Convert dicts back to Post objects for rendering
all_posts = [Post.from_dict(p) for p in all_posts_data]

print(f"Found {len(truly_new_posts)} new posts. Total posts: {len(all_posts)}")


# Generate (or regenerate) HTML files for all posts and ensure the
# page <link rel="canonical"> matches the feed-provided URL saved in
# posts_data.json (RSS is source-of-truth).
for p in all_posts:
    # Use the feed-provided link as canonical. Fall back to a Dev.to
    # constructed URL only if link is falsy for some reason.
    canonical = getattr(p, 'link', None) or f"https://dev.to/{DEVTO_USERNAME}/{p.slug}"
    html_out = PAGE_TMPL.render(
        title=p.title,
        canonical=canonical,
        description=p.description,
        date=p.date,
        content=p.content_html
    )
    (POSTS_DIR / f"{p.slug}.html").write_text(html_out, encoding="utf-8")
    print(f"Wrote: {p.slug}.html (canonical: {canonical})")

# Save the updated posts data for next run
save_posts_data(all_posts_data)

# Update the timestamp for the next run
set_last_run_timestamp()

# ----------------------------
# Build minimal comment pages (optional)
# ----------------------------
comment_items = load_comment_manifest()
if comment_items:
    pathlib.Path("comments").mkdir(exist_ok=True)
    for c in comment_items:
        title = "Comment note"
        desc = (c["context"] or "Comment note").strip()[:300]
        # For comment notes, canonical should point back to the original Dev.to URL
        html_page = COMMENT_NOTE_TMPL.render(
            title=title,
            canonical=c["url"],
            description=desc,
            context=html.escape(c["context"]) if c["context"] else "",
            url=c["url"]
        )
        pathlib.Path(c["local"]).write_text(html_page, encoding="utf-8")

# Use Dev.to profile as canonical for the index page
devto_profile = f"https://dev.to/{DEVTO_USERNAME}"
index_html = INDEX_TMPL.render(username=DEVTO_USERNAME, posts=all_posts, comments=comment_items, home=HOME, canonical=devto_profile)
pathlib.Path("index.html").write_text(index_html, encoding="utf-8")
pathlib.Path("robots.txt").write_text(ROBOTS_TMPL.format(home=HOME), encoding="utf-8")

smap = SITEMAP_TMPL.render(home=HOME, posts=all_posts, comments=comment_items)
pathlib.Path("sitemap.xml").write_text(smap, encoding="utf-8")

# Generated with the help of ChatGPT
