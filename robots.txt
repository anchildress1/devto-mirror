# Comprehensive robots.txt generated by scripts/generate_site.py
# This file allows a broad set of search engines, AI crawlers, archivers, SEO tools,
# and social preview agents. Edit as needed.

User-agent: *
Allow: /

# Major search engines
User-agent: Googlebot
Allow: /

User-agent: Google-Extended
Allow: /

User-agent: Bingbot
Allow: /

User-agent: DuckDuckBot
Allow: /

User-agent: Baiduspider
Allow: /

User-agent: Yandex
Allow: /

User-agent: Sogou
Allow: /

User-agent: Exabot
Allow: /

# AI and assistant crawlers
User-agent: GPTBot
Allow: /

User-agent: ClaudeBot
Allow: /

User-agent: Claude-Web
Allow: /

User-agent: PerplexityBot
Allow: /

User-agent: Bytespider
Allow: /

User-agent: HuggingFace
Allow: /

User-agent: BingPreview
Allow: /

# SEO, link research, and site-auditing bots
User-agent: AhrefsBot
Allow: /

User-agent: SemrushBot
Allow: /

User-agent: MJ12bot
Allow: /

User-agent: DotBot
Allow: /

# Archival crawlers
User-agent: CCBot
Allow: /

User-agent: ia_archiver
Allow: /

User-agent: archive.org_bot
Allow: /

# Social preview and embed bots
User-agent: facebookexternalhit
Allow: /

User-agent: Facebot
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: Slackbot
Allow: /

User-agent: LinkedInBot
Allow: /

User-agent: Pinterestbot
Allow: /

# Misc / vendor-specific
User-agent: Applebot
Allow: /

User-agent: Sistrix
Allow: /

User-agent: OpenWebSpider
Allow: /

# Default fallback (already covered above but kept for clarity)
User-agent: *
Allow: /

Sitemap: https://anchildress1.github.io/devto-mirror/sitemap.xml
